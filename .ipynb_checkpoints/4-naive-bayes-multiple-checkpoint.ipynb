{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how we can use bayes theorem to predict if observations fall into one class or another.  We did so using our formula of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E): \\frac{P(H)*P(E|H)}{P(EH) + P(EH^c)} = \\frac{P(H)*P(E|H)}{P(H)*P(E|H) + P(H^c)*P(E|H^c)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our key components in making our predictions were the prior, $P(H)$ and the likelihood $P(E|H)$.  We saw how to view this as a machine learning problem.  Given a certain feature, what is the probability of a target.  \n",
    "\n",
    "In this lesson, we'll explore this further, and consider what it means to have multiple features in predicting a target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving onto multiple features, let's review again what our formula is saying in it's simplest form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E): \\frac{P(HE)}{P(E)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formula states that the probability of the hypothesis given the evidence, equals the probability of the hypothesis and evidence being true divided by the probability that just the evidence is true.  And then we expand our numerator using the chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E): \\frac{P(HE)}{P(E)} = \\frac{P(H)P(E|H)}{P(E)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So focusing on the numerator, we found that the probability of the hypothesis and the evidence being true equals the probability of the hypothesis, times the probability of the evidence given the hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding another feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say that we use two features to find the probability of an outcome.  For example, let's return to the example of our iris flowers.  We'll use both sepal length **and sepal width** to predict if a flower is a Setosa or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./iris.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the probability a flower is a Setosa by finding $P(H|E,F)$, where $E$ represents a sepal length value, and $F$ represents a sepal width.  So, to calculate the probability $P(H|E, F)$, the probability of a setosa given both features, we use the following formula:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E, F): \\frac{P(HEF)}{P(EF)} = \\frac{P(H)P(E|H)P(F|H,F)}{P(EF)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So focusing on the numerator, we see that the probability of setosa occurring, and the features $E$ and $F$ occuring is the $P(H)$ times $P(E|H)$ times $P(F|H and F)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if $E$ represents sepal length of 5, and $F$ is sepal width of 3, then we would be calculating: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E, F) =\\frac{P(HEF)}{P(EF)} =  \\frac{P(H)P(E|H)P(F|H,F)}{P(EF)} = \\frac{P(setosa)P(l = 5|setosa)P(w = 3|setosa, l = 5)}{P(EF)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now focusing on the last term in the numerator, notice that our probability recognizes that the probability of our $width = 3$, changes if we know that we have a setosa, and the length is 5.  This is just the chain rule all over again. \n",
    "\n",
    "**But**, in practice, with *naive* bayes we assume that all of the feature values are independent of one another, changing our formula to: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E, F): \\frac{P(HEF)}{P(EF)} = \\frac{P(H)P(E|H)P(F|H)}{P(EF)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we add more and more features, our assumption of the features being independent of one another makes things easier.  And in terms of prediction, it works well in practice.  \n",
    "\n",
    "> This assumption of independence is what makes naive bayes, naive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing an Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see an example of Naive Bayes with multiple features working with our iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data[:, :2], columns = iris['feature_names'][:2])  # we only take the first two features.\n",
    "y = pd.Series(iris.target)\n",
    "\n",
    "setosa_df = X.loc[y[y == 0].index]\n",
    "not_setosa_df = X.loc[y[y != 0].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we loaded our data separated it by our setosas, and non setosas.  We selected the features of sepal length and sepal width for both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   sepal length (cm)  sepal width (cm)\n",
       " 0                5.1               3.5\n",
       " 1                4.9               3.0,\n",
       "     sepal length (cm)  sepal width (cm)\n",
       " 50                7.0               3.2\n",
       " 51                6.4               3.2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setosa_df[:2], not_setosa_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given an observation, we want to calculate something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(S|L, W) = \\frac{P(setosa)P(l = 5.1|setosa)P(w = 3.5|setosa, l = 5.1)}{P(LW)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know that $P(setosa) = .33$, and next filling out the numerator, we should find the probability of a specific length and width assuming we have a setosa.  To do that, we find the parameters of distribution of lengths and widths for a setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "setosa_length_mean, setosa_width_mean = setosa_df.mean()\n",
    "setosa_length_std, setosa_width_std = setosa_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can initialize the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs_setosa_length = scipy.stats.norm(setosa_length_mean, setosa_length_std)\n",
    "rvs_setosa_width = scipy.stats.norm(setosa_width_mean, setosa_width_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we could solve the numerator in our equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(S|L, W) = \\frac{P(setosa)P(l = 5.1|setosa)P(w = 3.5|setosa, l = 5.1)}{P(LW)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_setosa = .33\n",
    "p_set_len_five = rvs_setosa_length.pdf(5.1)\n",
    "p_set_width_three = rvs_setosa_width.pdf(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37256154850415774"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_setosa*p_set_len_five*p_set_width_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we find the denominator, by expanding it to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(S|L, W) = \\frac{P(setosa)P(l = 5.1|setosa)P(w = 3.5|setosa, l = 5.1)}{P(lw|setosa) + P(lw|setosa^c)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in the denominator, we already found as .372.  So now we just need to find the second term in the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This second term expands to: $P(setosa^c)P(l = 5.1|setosa^c)P(w = 3.5|setosa^c, l = 5.1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can essentially, repeat the steps to now find the probabilities assuming we do not have a setosa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_setosa_length_mean, not_setosa_width_mean = not_setosa_df.mean()\n",
    "not_setosa_length_std, not_setosa_width_std = not_setosa_df.std()\n",
    "\n",
    "rvs_not_setosa_length = scipy.stats.norm(not_setosa_length_mean, not_setosa_length_std)\n",
    "rvs_not_setosa_width = scipy.stats.norm(not_setosa_width_mean, not_setosa_width_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017521108897192"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_not_setosa = .67\n",
    "p_not_set_len_five = rvs_not_setosa_length.pdf(5.1)\n",
    "p_not_set_width_three = rvs_not_setosa_width.pdf(3.5)\n",
    "\n",
    "p_not_setosa*p_not_set_len_five*p_not_set_width_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H|E, F): \\frac{P(HEF)}{P(E)} = \\frac{P(H)P(E|H)P(F|H)}{P(E)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550835994250193"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_set_l_w = p_setosa*p_set_len_five*p_set_width_three\n",
    "p_l_w = (p_setosa*p_set_len_five*p_set_width_three + p_not_setosa*p_not_set_len_five*p_not_set_width_three)\n",
    "\n",
    "p_set_l_w/p_l_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that we get similar results if we fit our Gaussian naive bayes classifier with the same two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9753393])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "gnb.predict_proba(X[:1])[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about what makes the naive bayes formula naive.  The central idea is:\n",
    "\n",
    "1. Statistically (and logically), we should not consider our features as being independent of one another.  After all, knowing the value of one feature like sepal length changes the probability of observing another feature like sepal width.\n",
    "\n",
    "2. But, in practice we can more easily train our classifier if we assume independence.  And assuming independence still leads to fairly accurate classifier.\n",
    "\n",
    "So this assumption of independence leads to the following formula:\n",
    "\n",
    "$P(H|E, F): \\frac{P(HEF)}{P(EF)} = \\frac{P(H)P(E|H)P(F|H)}{P(EF)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And applied to our example of the setosa is the following:\n",
    "    \n",
    "$P(S|L, W) = \\frac{P(setosa)P(l = 5.1|setosa)P(w = 3.5|setosa, l = 5.1)}{P(lw|setosa) + P(lw|setosa^c)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
